{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONDhgqCRXdDpYhlUwHzAyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya161205/work/blob/main/micro_embed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAKING EMBEDDING MODEL FROM SCRATCH**"
      ],
      "metadata": {
        "id": "LYTR2XeA7ayc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#PART 1: THE TOKENIZER (Byte Pair Encoding)\n",
        "\n",
        "WHY: Real models don't split by space. They learn sub-words.\n",
        "\n",
        "e.g., \"unknown\" -> \"un\" + \"known\". This solves the \"Out of Vocabulary\" error."
      ],
      "metadata": {
        "id": "dsbVSNWWaWN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement simple Byte-Pair-Encoding algorithm"
      ],
      "metadata": {
        "id": "o1ViY9QBabYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding (BPE) is a subword tokenization algorithm that repeatedly merges the most frequent adjacent character pairs to form new tokens.\n",
        "It helps reduce vocabulary size and handle rare or unseen words.\n",
        "\n",
        "Example:\n",
        "Text: low lower → start as l o w | l o w e r → frequent pair l o merges to lo → tokens become lo w | lo w e r."
      ],
      "metadata": {
        "id": "tIcQWHCwampU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NDl7wM4VY-38"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleBPE:\n",
        "\n",
        "  def __init__(self,num_merges=10):\n",
        "    self.merges = {}\n",
        "    self.vocab = {}\n",
        "    self.num_merges = num_merges\n",
        "\n",
        "\n",
        "  def get_stats(self, vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    #pairs = collections.defaultdict(int) creates a dictionary with default value 0 for any missing key\n",
        "    for word, freq in vocab.items():\n",
        "      symbols = word.split()\n",
        "      for i in range(len(symbols)-1):\n",
        "        pairs[symbols[i],symbols[i+1]] += freq\n",
        "      return pairs\n",
        "\n",
        "  #This function merges a chosen frequent symbol pair (bigram) into a single token across the vocabulary.\n",
        "  def merge_vocab(self,pair,v_in):\n",
        "    v_out = {}\n",
        "\n",
        "    bigram = ' '.join(pair) #Converts ('l','o') → \"l o\" (how it appears in vocab).\n",
        "    replacement = ''.join(pair) #Converts ('l','o') → \"lo\" (merged token).\n",
        "\n",
        "    for word in v_in:\n",
        "      w_out = word.replace(bigram,replacement)\n",
        "      v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "  def train(self, text):\n",
        "\n",
        "    raw_words = text.split()\n",
        "    vocab = {\" \".join(list(w)) + \" </w>\": 1 for w in raw_words}\n",
        "\n",
        "    for i in range(self.num_merges):\n",
        "      pairs = self.get_stats(vocab)\n",
        "      if not pairs: break\n",
        "      best = max(pairs, key = pairs.get)\n",
        "      self.merges[best]= ''.join(best) #store the merge\n",
        "      vocab = self.merge_vocab(best,vocab) #merge the pair\n",
        "\n",
        "    #making the final mapping (Token - > ID)\n",
        "    unique_tokens = set(\" \".join(vocab.keys()).split())\n",
        "    self.vocab = {token: i for i, token in enumerate(unique_tokens)}\n",
        "    #add special [UNK] token for unknowns\n",
        "    self.vocab[\"[UNK]\"]= len(self.vocab)\n",
        "\n",
        "  def encode(self,text):\n",
        "    word = \" \".join(list(text)) + \"</w>\"\n",
        "    for pair, replacement in self.merges.items():\n",
        "      bigram = ' '.join(pair)\n",
        "      if bigram in word:\n",
        "        word = word.replace(bigram, replacement)\n",
        "    tokens = word.split()\n",
        "    return [self.vocab.get(t,self.vocab[\"[UNK]\"]) for t in tokens]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C7lj6Bk0ZEn2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first two functions in the BPE class do the following.\n",
        "\n",
        "Initial vocabulary:\n",
        "n e w : 6\n",
        "n e w e r : 3\n",
        "\n",
        "Pair counts computed by get_stats:\n",
        "(n, e) = 9\n",
        "(e, w) = 9\n",
        "(w, e) = 3\n",
        "(e, r) = 3\n",
        "\n",
        "Chosen pair to merge:\n",
        "(n, e)\n",
        "\n",
        "After applying merge_vocab:\n",
        "n e w     becomes ne w\n",
        "n e w e r becomes ne w e r\n",
        "\n",
        "Updated vocabulary:\n",
        "ne w : 6\n",
        "ne w e r : 3\n"
      ],
      "metadata": {
        "id": "-C2OR8YTfPBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###this is the step by step flow of BPE\n",
        "\n",
        "Input           → n e w e r </w>\n",
        "\n",
        "After merges    → new e r </w>\n",
        "\n",
        "Tokens          → [\"new\", \"e\", \"r\", \"</w>\"]\n",
        "\n",
        "IDs             → [id(new), id(e), id(r), id(</w>)]\n"
      ],
      "metadata": {
        "id": "G-NhwxPljwMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PART 2: THE ATTENTION LAYER (The \"Brain\")\n",
        "\n",
        "WHY: This replaces the simple LinearLayer.\n",
        "\n",
        "It allows words to \"vote\" on their meaning based on context."
      ],
      "metadata": {
        "id": "jKHtv_cak5oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionLayer:\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        self.d_k = head_dim\n",
        "        # Q, K, V matrices (Randomly initialized)\n",
        "        self.W_q = np.random.randn(embed_dim, head_dim) * 0.01\n",
        "        self.W_k = np.random.randn(embed_dim, head_dim) * 0.01\n",
        "        self.W_v = np.random.randn(embed_dim, head_dim) * 0.01\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 1. Linear Projections\n",
        "        Q = np.dot(x, self.W_q)\n",
        "        K = np.dot(x, self.W_k)\n",
        "        V = np.dot(x, self.W_v)\n",
        "\n",
        "        # 2. Scaled Dot-Product Attention\n",
        "        # Scores = (Q * K.T) / sqrt(d_k)\n",
        "        scores = np.dot(Q, K.T) / np.sqrt(self.d_k)\n",
        "\n",
        "        # 3. Softmax\n",
        "        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "        weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "\n",
        "        # 4. Contextualize\n",
        "        context = np.dot(weights, V)\n",
        "        #IMP->Each token collects information from all tokens, weighted by how much it should “pay attention” to them.\n",
        "        return context"
      ],
      "metadata": {
        "id": "tWEYU0-JZY3J"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 3: THE MODEL WRAPPER"
      ],
      "metadata": {
        "id": "lrPbNxgrmBOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniTransformer:\n",
        "  def __init__(self, vocab_size,embed_dim):\n",
        "    #static embeddings->work as lookup table\n",
        "    self.embeddings = np.random.randn(vocab_size, embed_dim)*0.01\n",
        "    #attention mechanism\n",
        "    self.attention = SelfAttentionLayer(embed_dim,embed_dim)\n",
        "\n",
        "  def get_sentence_vector(self, token_ids):\n",
        "        \"\"\"\n",
        "        Full Forward Pass: IDs -> Static -> Attention -> Mean Pooling\n",
        "        \"\"\"\n",
        "        # Lookup Static Vectors\n",
        "        static_vecs = self.embeddings[token_ids]# Shape: [Seq_Len, Embed_Dim]\n",
        "\n",
        "        # Apply Attention\n",
        "        context_vecs = self.attention.forward(static_vecs)# Shape: [Seq_Len, Embed_Dim] (Contextualized)\n",
        "\n",
        "        # Mean Pooling (Crude way to get 1 vector for the whole sentence)\n",
        "        sent_vec = np.mean(context_vecs, axis=0)# We average all word vectors to get the \"Sentence Meaning\"\n",
        "\n",
        "        # Normalize to unit sphere\n",
        "        return sent_vec / (np.linalg.norm(sent_vec) + 1e-9)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c-gq1xwjl-S4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 4: DATA AUGMENTER\n",
        "\n",
        "Helps to learn similarity between sentences"
      ],
      "metadata": {
        "id": "1jVo9LJ12d9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmenter:\n",
        "    def augment(self, sentence):\n",
        "        words = sentence.split() # Simple split for augmentation logic\n",
        "        if len(words) > 1:\n",
        "            remove_idx = random.randint(0, len(words) - 1)\n",
        "            new_words = words[:remove_idx] + words[remove_idx+1:]\n",
        "            return \" \".join(new_words)\n",
        "        return sentence"
      ],
      "metadata": {
        "id": "yBcoQXHJqO64"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 5: TRAINING LOOP (The \"Pipeline\")"
      ],
      "metadata": {
        "id": "U6Hmmh0o2pG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1->Data and tokenizer setup\n",
        "\n",
        "raw_text = \"the bank of money and the river bank\"\n",
        "\n",
        "#now we train BPE on this text\n",
        "tokenizer = SimpleBPE(num_merges=10)\n",
        "tokenizer.train(raw_text)\n",
        "\n",
        "#initializing\n",
        "model = MiniTransformer(len(tokenizer.vocab),embed_dim=8)\n",
        "augmenter = DataAugmenter()\n",
        "\n",
        "print(f\"vocab: {tokenizer.vocab}\")\n",
        "print(\"training started\")\n",
        "\n",
        "\n",
        "#Step 2-> Training phase\n",
        "# (Note: Full backprop for Attention/Softmax is too complex for this snippet.\n",
        "# We will simulate the Forward Pass and Loss Calculation to show data flow.)\n",
        "\n",
        "sentences = [\"river bank\", \"money bank\"]\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for sent in sentences:\n",
        "        # --- A. PREPARE TRIPLETS ---\n",
        "        # Anchor: \"river bank\"\n",
        "        # Positive: \"bank\" (Augmented)\n",
        "        # Negative: \"money bank\" (Hard Negative)\n",
        "\n",
        "        anchor_text = sent\n",
        "        pos_text = augmenter.augment(sent)\n",
        "        neg_text = \"money bank\" if \"river\" in sent else \"river bank\"\n",
        "\n",
        "        # --- B. TOKENIZE & EMBED ---\n",
        "        # Convert strings to Lists of IDs\n",
        "        ids_anc = tokenizer.encode(anchor_text)\n",
        "        ids_pos = tokenizer.encode(pos_text)\n",
        "        ids_neg = tokenizer.encode(neg_text)\n",
        "\n",
        "        # Get Sentence Vectors (Forward Pass through Attention)\n",
        "        v_anc = model.get_sentence_vector(ids_anc)\n",
        "        v_pos = model.get_sentence_vector(ids_pos)\n",
        "        v_neg = model.get_sentence_vector(ids_neg)\n",
        "\n",
        "        # --- C. COMPUTE LOSS (InfoNCE) ---\n",
        "        # Dot Products\n",
        "        sim_pos = np.dot(v_anc, v_pos)\n",
        "        sim_neg = np.dot(v_anc, v_neg)\n",
        "\n",
        "        # Softmax & NLL\n",
        "        scores = np.array([sim_pos, sim_neg])\n",
        "        exp_scores = np.exp(scores - np.max(scores)) # Stability\n",
        "        probs = exp_scores / np.sum(exp_scores)\n",
        "\n",
        "        loss = -np.log(probs[0]) # We want index 0 (Positive) to be 1.0\n",
        "        total_loss += loss\n",
        "\n",
        "        # (Optimization Step Omitted for brevity: requires complex chain rule)\n",
        "        # In a real framework, optimizer.step() happens here.\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WVvscGn2nbI",
        "outputId": "e1e2c6db-0048-4dee-87da-a3171ec33a5c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab: {'v': 0, 'o': 1, 'a': 2, 'e': 3, 'm': 4, 'r': 5, 'the</w>': 6, 'n': 7, '</w>': 8, 'y': 9, 'k': 10, 'i': 11, 'f': 12, 'b': 13, 'd': 14, '[UNK]': 15}\n",
            "training started\n",
            "Epoch 0: Loss 1.0415\n",
            "Epoch 1: Loss 1.0415\n",
            "Epoch 2: Loss 1.0415\n",
            "Epoch 3: Loss 1.2781\n",
            "Epoch 4: Loss 1.0938\n",
            "Epoch 5: Loss 1.2781\n",
            "Epoch 6: Loss 1.0938\n",
            "Epoch 7: Loss 1.0415\n",
            "Epoch 8: Loss 1.2781\n",
            "Epoch 9: Loss 1.2781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# VERIFICATION (Does Attention Work?)\n",
        "\n",
        "# Let's see if the word \"bank\" has different vectors in different contexts\n",
        "ids_river = tokenizer.encode(\"river bank\")\n",
        "ids_money = tokenizer.encode(\"money bank\")\n",
        "\n",
        "# Get vector for the sentence \"river bank\"\n",
        "vec_river_bank = model.get_sentence_vector(ids_river)\n",
        "# Get vector for the sentence \"money bank\"\n",
        "vec_money_bank = model.get_sentence_vector(ids_money)\n",
        "\n",
        "print(f\"Vector 'River Bank' (First 4 dims): {np.round(vec_river_bank[:4], 2)}\")\n",
        "print(f\"Vector 'Money Bank' (First 4 dims): {np.round(vec_money_bank[:4], 2)}\")\n",
        "\n",
        "similarity = np.dot(vec_river_bank, vec_money_bank)\n",
        "print(f\"Similarity between them: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rypg_vA4RvJ",
        "outputId": "e3185305-2d7c-4a47-9410-3c5e4cf10f1c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ATTENTION CHECK ---\n",
            "Vector 'River Bank' (First 4 dims): [-0.23  0.25  0.1  -0.03]\n",
            "Vector 'Money Bank' (First 4 dims): [ 0.35  0.3   0.03 -0.6 ]\n",
            "Similarity between them: 0.5210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets implement this using Pytorch to unleash the accuracy"
      ],
      "metadata": {
        "id": "gG_5pFQp5tsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "UnZY02Bp6AoL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. THE MODEL (Identical Logic, Auto-Calculus)\n"
      ],
      "metadata": {
        "id": "tYo_xBlq6CZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # Static Embeddings (The Lookup Table)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Q, K, V Projections (The Linear Layers we built manually)\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "    def get_sentence_vector(self, token_ids):\n",
        "        # 1. Embed inputs (Shape: [Seq_Len, Embed_Dim])\n",
        "        # We use torch.tensor to wrap inputs\n",
        "        x = self.embedding(token_ids)\n",
        "\n",
        "        # 2. Self-Attention Logic (The \"Brain\")\n",
        "        # A. Create Q, K, V\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # B. Dot Product Scores (Q * K.T)\n",
        "        # torch.matmul is exactly np.dot\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "\n",
        "        # C. Scale\n",
        "        d_k = x.size(-1)\n",
        "        scores = scores / (d_k ** 0.5)\n",
        "\n",
        "        # D. Softmax (The \"Jacobian\" part handled automatically)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # E. Contextualize (Weights * V)\n",
        "        context = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # 3. Mean Pooling (Sentence Vector)\n",
        "        # Average all words to get 1 vector\n",
        "        sent_vec = torch.mean(context, dim=0)\n",
        "\n",
        "        # Normalize (L2 Norm)\n",
        "        return F.normalize(sent_vec, p=2, dim=0)\n"
      ],
      "metadata": {
        "id": "_i6iHOTc6SFm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. TRAINING LOOP"
      ],
      "metadata": {
        "id": "7sbjG4Uw6hFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size = 10\n",
        "ids_river_bank = torch.tensor([0, 1])  # \"river bank\"\n",
        "ids_bank_aug   = torch.tensor([1])     # \"bank\" (Positive)\n",
        "ids_money_bank = torch.tensor([2, 1])  # \"money bank\" (Negative)\n",
        "\n",
        "model = MiniTransformer(vocab_size, embed_dim=8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1) # The \"Teacher\"\n",
        "\n",
        "print(\"Training PyTorch Transformer...\\n\")\n",
        "\n",
        "for epoch in range(20):\n",
        "    optimizer.zero_grad() # Reset gradients-> IMPORTANT\n",
        "\n",
        "    # --- Forward Pass ---\n",
        "    # 1. Get Vectors\n",
        "    anchor = model.get_sentence_vector(ids_river_bank)\n",
        "    positive = model.get_sentence_vector(ids_bank_aug)\n",
        "    negative = model.get_sentence_vector(ids_money_bank)\n",
        "\n",
        "    # 2. Compute Similarity (Dot Product)\n",
        "    sim_pos = torch.dot(anchor, positive)\n",
        "    sim_neg = torch.dot(anchor, negative)\n",
        "\n",
        "    # 3. Compute Loss (InfoNCE)\n",
        "    # Stack scores: [Positive, Negative]\n",
        "    logits = torch.stack([sim_pos, sim_neg])\n",
        "    # Target is index 0 (The Positive Pair)\n",
        "    target = torch.tensor([0])\n",
        "\n",
        "    # CrossEntropyLoss combines Softmax + Log + NLL automatically\n",
        "    loss = F.cross_entropy(logits.unsqueeze(0), target)\n",
        "\n",
        "    # --- Backward Pass (The Magic) ---\n",
        "    loss.backward() # Calculates ALL derivatives (Jacobians, Chain Rule)\n",
        "    optimizer.step() # Updates weights\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbuNdkkK5zmZ",
        "outputId": "62c25402-fc7e-4e57-bbbf-c07e361807d0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training PyTorch Transformer...\n",
            "\n",
            "Epoch 0: Loss 0.6697\n",
            "Epoch 2: Loss 0.1355\n",
            "Epoch 4: Loss 0.1289\n",
            "Epoch 6: Loss 0.1287\n",
            "Epoch 8: Loss 0.1289\n",
            "Epoch 10: Loss 0.1287\n",
            "Epoch 12: Loss 0.1282\n",
            "Epoch 14: Loss 0.1278\n",
            "Epoch 16: Loss 0.1275\n",
            "Epoch 18: Loss 0.1272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. RESULTS"
      ],
      "metadata": {
        "id": "omUrWuTM6y_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Final Check ---\")\n",
        "# Re-calculate vectors without updating weights\n",
        "v_anchor = model.get_sentence_vector(ids_river_bank)\n",
        "v_neg = model.get_sentence_vector(ids_money_bank)\n",
        "\n",
        "sim = torch.dot(v_anchor, v_neg).item()\n",
        "print(f\"Similarity: {sim:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niZEIrWG54IA",
        "outputId": "e998f92c-792d-40de-f628-87d814c5c364"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Check ---\n",
            "Similarity: -0.9993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tbc3GHrH633j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}